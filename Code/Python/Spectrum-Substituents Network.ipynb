{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n",
      "0.23.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from keras.layers import Input, Dense, Lambda, concatenate, Conv1D, Concatenate, Flatten, MaxPooling1D\n",
    "from keras.models import Model\n",
    "import pandas as pd\n",
    "from itertools import islice\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up variables for constants such as absolute datapaths and the desired valdiation fraction split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Oxane\n"
     ]
    }
   ],
   "source": [
    "classyfire_datapath = \"G:\\\\Dev\\\\Data\\\\For Substituent GNPS ALL\\\\GNPS Python Master\\\\Final Data.txt\"\n",
    "filtered_classyfire_datapath = \"G:\\\\Dev\\\\Data\\\\For Substituent GNPS ALL\\\\GNPS Python Master\\\\Final Filtered Data.txt\"\n",
    "substituents_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\GNPS_substituents.txt\"\n",
    "filtered_substituents_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\GNPS_filtered_substituents.txt\"\n",
    "substituents_names_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\GNPS_substituents_legend.txt\"\n",
    "filtered_substituents_names_path = \"G:\\\\Dev\\\\Data\\\\filtered_top_substituents_legend.txt\"\n",
    "substituents_auc_results_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\substituents_auc_results.txt\"\n",
    "num_samples = 9238\n",
    "numSubstituents = 381\n",
    "val_fraction = 0.1\n",
    "default_dpi = plt.rcParamsDefault['figure.dpi']\n",
    "plt.rcParams['figure.dpi'] = default_dpi*1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are methods used to load in fragment spectra and fingerprint data from files stored in the absolute paths specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a master file containing peak intensities for all molecules.\n",
    "# Each molecule's spectrum is added as a 1000 element row to a Pandas dataframe\n",
    "# The dataframe is then converted into a numpy array for use as Keras Input.\n",
    "# Include the option of adding additonal features to each molecule (mass_shifts variable)\n",
    "def load_master_file(path, mass_shifts = 0, number_of_bins = 1000):\n",
    "    MAX_MASS = 1000\n",
    "    BIN_SIZE = 1\n",
    "    NUM_FEATURES = mass_shifts\n",
    "    mol_all = np.loadtxt(path, dtype=\"U30\") # Get master file in as numpy array\n",
    "    \n",
    "    mol_ids = np.unique(mol_all[:, 0])  # Trim duplicate filename rows, store unique filenames\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in mol_ids).\n",
    "    intensities = pd.DataFrame(0.0, index = mol_ids, columns=range((number_of_bins//BIN_SIZE)+NUM_FEATURES), dtype=float)\n",
    "    \n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in mol_all:\n",
    "        intensities.at[row[0], float(row[1])-1] = float(row[2])\n",
    "    \n",
    "    # Convert populated dataframe into a numpy array for use by neural networks.\n",
    "    np_matrix = intensities.values\n",
    "    return np_matrix\n",
    "\n",
    "# Load a master file containing CDK fingerprints for all molecules.\n",
    "# Each molecules CDK bit set is added as a 320 element array to a Pandas dataframe.\n",
    "def load_substituents_master():\n",
    "    BITS = 2098  # Total number of bits in fingerprint\n",
    "\n",
    "    fp_all = np.loadtxt(substituents_path, dtype=\"U30\") # Get master file as numpy array of Strings\n",
    "    fp_ids = np.unique(fp_all[:, 0]) # Trim duplicate filename rows, store unique filenames\n",
    "\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in fp_ids).\n",
    "    substituents = pd.DataFrame(0, index = fp_ids, columns=range(BITS), dtype=int)\n",
    "\n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in fp_all:\n",
    "        substituents.at[row[0], int(row[1])] = int(row[2])\n",
    "\n",
    "    # Convert populated dataframe into a numpy array for use as output by neural networks.\n",
    "    \n",
    "    np_matrix = substituents\n",
    "    return np_matrix\n",
    "\n",
    "def load_filtered_substituents_master():\n",
    "    BITS = 381  # Total number of bits in fingerprint\n",
    "\n",
    "    fp_all = np.loadtxt(filtered_substituents_path, dtype=\"U30\") # Get master file as numpy array of Strings\n",
    "    fp_ids = np.unique(fp_all[:, 0]) # Trim duplicate filename rows, store unique filenames\n",
    "\n",
    "    # Construct empty Pandas dataframe of correct size.\n",
    "    # Number of rows is equal to the number of unique molecules (found in fp_ids).\n",
    "    substituents = pd.DataFrame(0, index = fp_ids, columns=range(BITS), dtype=int)\n",
    "    \n",
    "    # Populate the dataframe using each molecule's filename to place data in the correct row.\n",
    "    for row in fp_all:\n",
    "        if int(row[1]) > 381:\n",
    "            print(row[1])\n",
    "        substituents.at[row[0], int(row[1])] = int(row[2])\n",
    "\n",
    "    # Convert populated dataframe into a numpy array for use as output by neural networks.\n",
    "    \n",
    "    np_matrix = substituents\n",
    "    return np_matrix\n",
    "\n",
    "# Load the names of all substituents included in the correct order\n",
    "# This is used for boxplots, when performance metrics for individual substituent are calculated.\n",
    "def load_substituent_legend(path):\n",
    "    substituent_legend = []\n",
    "    # Open file containing substituent names.\n",
    "    with open(path, 'r') as f:\n",
    "        # Add each name to the list of substituent names.\n",
    "        lines = list(islice(f, 0, None))\n",
    "        for line in lines:\n",
    "            substituent_legend.append(line[:-1])\n",
    "    return substituent_legend\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below methods create and train various neural networks when provided with valid input and output data. They allow for specifying the number of epochs the network is to be trained for and, in some cases, the learning rate. Trained models are returned and can be used to predict on test data and thereby be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Sotchastic Gradient Descent object from Keras to allow for tweaking its learning rate.\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# A simplified spectrum-fingeprrint encoder.\n",
    "# Structure: (Input)1000-500-200-2098(Output)\n",
    "def simplified_substituent_model(x_train_spectra, x_train_substituents, epochs=100):\n",
    "    # Create input based on the provided x_train data structure.\n",
    "    input_layer = Input(shape=(x_train_spectra.shape[1],))\n",
    "    # Since output is not the same as input, we obtain its shape separately.\n",
    "    output_dims = x_train_substituents.shape[1]\n",
    "    print(output_dims)\n",
    "    # Create the encoding layers using functional API.\n",
    "    l = input_layer\n",
    "    l = Dense(500, activation='relu')(l)\n",
    "    \n",
    "    # Linear activation ensures that values can be negative (necessary for sigmoid to function)\n",
    "    l = Dense(200, activation='linear')(l)\n",
    "    \n",
    "    # Save reference to latent space\n",
    "    latent_space = l\n",
    "    \n",
    "    # Sigmoid activation to get outputs between 0 and 1. This is done because the output fingerprint is a set of bits (0 or 1).\n",
    "    l2 = Dense(output_dims, activation='sigmoid')(l)\n",
    "    \n",
    "    #Reference for output layer\n",
    "    out_layer = l2\n",
    "\n",
    "    auto_model = Model(input=input_layer, output=out_layer)\n",
    "    \n",
    "    # Set SGD learning rate = 0.05 and compile model with binary_crossentropy as loss function.\n",
    "    sgd = SGD(lr=0.05)\n",
    "    auto_model.compile(loss='binary_crossentropy', optimizer=sgd)\n",
    "    \n",
    "    # Train the model for the specified number of epochs, using the specified validation fraction.\n",
    "    autoencoder_train = auto_model.fit(x_train_spectra, x_train_substituents, shuffle=False, validation_split = 0.1, epochs=epochs)\n",
    "    \n",
    "    # Loss Plots\n",
    "    plot_loss(autoencoder_train, epochs)\n",
    "    \n",
    "    return auto_model # Return model, now trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes as input a trained neural network model and extracts its history variable.\n",
    "# It then uses it to graph the model's loss and validation loss over the training epochs\n",
    "# The epochs paramter is used for plotting the x axis.\n",
    "def plot_loss(fitted_model, epochs):\n",
    "    # Extract loss values for the training and validation sets.\n",
    "    loss = fitted_model.history['loss']\n",
    "    val_loss = fitted_model.history['val_loss']\n",
    "    # Create x axis variables.\n",
    "    epochs_label = epochs\n",
    "    epochs = range(epochs)\n",
    "\n",
    "    #Plot both losses.\n",
    "    plt.figure()\n",
    "    plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "    plt.plot(epochs, val_loss,'r', label='Validation loss')\n",
    "    plt.title('Training and Validation Loss for ' + str(epochs_label) + ' epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "# Takes a actual and predicted fingerprint values and computes the area under the Roc curve for each substructure.\n",
    "# For each subtructure, also calculates AUC when the actual values are scrambled.\n",
    "# Return two numpy arrays: one containing AUC metrics for all susbtructures, one containing each permutation's\n",
    "# AUC scores for each susbtructure.\n",
    "from sympy.utilities.iterables import multiset_permutations\n",
    "def compute_auc(bits, true, pred, num_samples=0, permutations=500):\n",
    "    val_start_index = int(num_samples-(num_samples*val_fraction)-1) # Index where validation samples begin.\n",
    "    \n",
    "    num_permutations = permutations  # Number of permutations to compute AUC scores for. \n",
    "    \n",
    "    # Create structured array to hold statistics for each fingerprint.\n",
    "    dtype = [('fp_id', int),('nonzeros', int), ('auc', float), ('auc_percent', float)]\n",
    "    mol_stats = np.zeros((bits,), dtype=dtype)\n",
    "\n",
    "    # Create array to hold permutation AUC scores for plotting.\n",
    "    perm_scores = np.zeros((bits, num_permutations))\n",
    "    val_start_index = 0\n",
    "    for fp_id in range(true.shape[1]): # For every substructure\n",
    "        nonzero_vals = np.count_nonzero(true[val_start_index:, fp_id]) # Count number of nonzero values\n",
    "        if nonzero_vals > 0 and nonzero_vals < true[val_start_index:, fp_id].size:  # If there are no 1s or no 0s, can't compute.\n",
    "            # Compute actual AUC score using only the validation fraction of the dataset.\n",
    "            fp_true = true[val_start_index:, fp_id]\n",
    "            fp_pred = pred[val_start_index:, fp_id]\n",
    "            score = metrics.roc_auc_score(fp_true, fp_pred)\n",
    "\n",
    "            # Compute AUC scores for permutations and compare to actual.\n",
    "            counter = 0         \n",
    "            for i in range(num_permutations):\n",
    "                permutation = np.random.permutation(fp_true)\n",
    "                perm_score = metrics.roc_auc_score(permutation, fp_pred)\n",
    "                perm_scores[fp_id, i] = perm_score\n",
    "                # Count how many permutations have a higer AUC score than actual data.\n",
    "                if perm_score >= score:\n",
    "                    counter = counter + 1\n",
    "            # Calculate % of scrambled values with higher AUC score than actual AUC\n",
    "            percentage = (counter/num_permutations)*100\n",
    "        # Update structured array with data or non values if no AUC could be calculated.\n",
    "            mol_stats[fp_id] = fp_id, nonzero_vals, score, percentage\n",
    "        else:\n",
    "            mol_stats[fp_id] = (fp_id, nonzero_vals, 0, 100)\n",
    "        \n",
    "    # Permutations take a while, print statement to say when finished.\n",
    "    print(\"Compute AUC Done\")\n",
    "    return mol_stats, perm_scores\n",
    "\n",
    "\n",
    "# Takes a set of AUC scores and permutation AUC scores (normally output by compute_auc above) an uses them\n",
    "# to draw boxplots for specified susbtructures. Actual AUC is plotted as a coloured dot.\n",
    "plt.rcParams['figure.dpi'] = default_dpi*2.2\n",
    "def boxplots(real_stats, perm_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(substituent_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "\n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures in colour.\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "# Takes a set of AUC scores and permutation AUC scores and uses them to draw boxplots for specified substructures\n",
    "# Actual AUC is plotted as a coloured dot. A separate set of AUC scores\n",
    "# computed for prediction from a different model is also plotted for comparison\n",
    "def tandem_boxplots(real_stats, perm_stats, exp_stats, sample_fps):\n",
    "    index = sample_fps['fp_id']  # Grab id of each substructure to be plotted, used as index in parallel arrays\n",
    "    names = np.array(substituent_names)[index]  # Grab name of each susbtructure to be plotted.\n",
    "  \n",
    "    plt.rcParams.update({'font.size': 6})\n",
    "    plt.figure()\n",
    "    plt.boxplot(perm_stats[index].T, vert=False, labels = names) # Boxplot permutation AUC scores\n",
    "    plt.scatter(real_stats[index]['auc'], range(1, len(index)+1)) # Scatter plot actual AUC scores for substructures\n",
    "    plt.scatter(exp_stats[index]['auc'], range(1, len(index)+1), color = 'r') # Scatter plot AUC scores to be compared to.\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Given the AUC statistics derived from two separate models, it comapres the two models' performance\n",
    "# Creates a bar chart comparing substructures above an AUC threshold and draws boxplots for each model's best and worst\n",
    "# performing substructures.\n",
    "# Usually compares an experimental model's AUC to a baseline (e.g. the basic fingerprint encoder)\n",
    "def evaluate(base_stats, base_perm_scores, exp_stats, exp_perm_scores):\n",
    "    # Sort molecules in ascending order of baseline AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((base_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((base_stats['auc']) < 0.5)\n",
    "    ordered_base = np.sort(base_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_base[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_base[-5:])\n",
    "    \n",
    "    # Plot number of substructures with AUC scores above 0.7 and above 0.5 for both data sets\n",
    "    base_above_07 = len(np.where((base_stats['auc'] >= 0.7))[0])\n",
    "    exp_above_07 = len(np.where((exp_stats['auc'] >= 0.7))[0])\n",
    "    base_above_05 = len(np.where((base_stats['auc'] >= 0.5))[0])\n",
    "    exp_above_05 = len(np.where((exp_stats['auc'] >= 0.5))[0])\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    index = np.arange(2)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.5\n",
    "    ax.bar(index, (base_above_05, base_above_07), bar_width, alpha=opacity, color='b', label='Baseline')\n",
    "    ax.bar(index+bar_width, (exp_above_05, exp_above_07), bar_width, alpha=opacity, color='r', label='Experiment')\n",
    "    \n",
    "    ax.set_xlabel('AUC Threshold')\n",
    "    ax.set_ylabel('Number of Substituents')\n",
    "    ax.set_title('AUC Score Comparison')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(('Above 0.5', 'Above 0.7'))\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Boxplots of sample substructures for both data sets\n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, base_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "     # Sort molecules in ascending order of experimental AUC score, keeping only molecules with AUC scores above 0.5\n",
    "    normal_auc = np.where((exp_stats['auc'] > 0.5))\n",
    "    abnormal_auc = np.where((exp_stats['auc']) < 0.5)\n",
    "    ordered_exp = np.sort(exp_stats[normal_auc], order='auc', axis=0)[::-1]\n",
    "    \n",
    "    # Take top 30 and bottom 5 substructures by AUC score to use for boxplots.\n",
    "    sample_fps = ordered_exp[:30]\n",
    "    sample_fps = np.append(sample_fps, ordered_exp[-5:])\n",
    "    \n",
    "    boxplots(base_stats, base_perm_scores, sample_fps)\n",
    "    boxplots(exp_stats, exp_perm_scores, sample_fps)\n",
    "    tandem_boxplots(base_stats, exp_perm_scores, exp_stats, sample_fps)\n",
    "    \n",
    "\n",
    "# Given a matrix of layer weights, plots them in a Hinton diagram: each weight is a box\n",
    "# Box size is indicates absolute value, box colour indicates sign (white for positive, black for negative)\n",
    "# Adapted from matplotlib documentation.\n",
    "def hinton(matrix, max_weight=None, ax=None):\n",
    "    ax = ax if ax is not None else plt.gca()\n",
    "    # Find maximum weight in matrix.\n",
    "    if not max_weight:\n",
    "        max_weight = 2 ** np.ceil(np.log(np.abs(matrix).max()) / np.log(2))\n",
    "\n",
    "    ax.patch.set_facecolor('gray')\n",
    "    ax.set_aspect('equal', 'box')\n",
    "    ax.xaxis.set_major_locator(plt.NullLocator())\n",
    "    ax.yaxis.set_major_locator(plt.NullLocator())\n",
    "\n",
    "    # Plot weights as black or white boxes.\n",
    "    for (x, y), w in np.ndenumerate(matrix):\n",
    "        color = 'white' if w > 0 else 'black'\n",
    "        size = np.sqrt(np.abs(w) / max_weight)\n",
    "        rect = plt.Rectangle([x - size / 2, y - size / 2], size, size,\n",
    "                             facecolor=color, edgecolor=color)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    ax.autoscale_view()\n",
    "    ax.invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following methods are quick ways to train multiple times using different training-validation splits. Used when we want means, error bars and statistical tests for model comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains the basic substituent encoder using 10 different validation-training splits.\n",
    "# Computes AUC scores for each split and stores them as a separate file.\n",
    "# Takes path to store files in as a parameter, as well as the name of the test.\n",
    "def train_diff_splits(path, name, splits=10):\n",
    "    # Extract permuted indices for dataset.\n",
    "    index_path = \"G:\\\\Dev\\\\Data\\\\GNPS ALL Substituent Validation Split Permutations.txt\"\n",
    "    permuted_indices = np.loadtxt(index_path, dtype=int, delimiter=',')\n",
    "    epochs = 100\n",
    "    path = path + name + \" \"\n",
    "    #List to store AUC scores if we want to use them right away instead of loading from files.\n",
    "    experiment_stats = []\n",
    "    for i in range(splits):\n",
    "        # Create filepath for this training session.\n",
    "        curr_path = path + str(i) + \".txt\"\n",
    "        # Use permuted indices to create permuted array of input data.\n",
    "        x_train_dense = spectra[permuted_indices[:, i]]\n",
    "        x_train_dense = np.log(x_train_dense+1)\n",
    "        x_train_substituents = substituents.values[permuted_indices[:, i]]\n",
    "        # Train a basic model.\n",
    "        enc_basic = simplified_substituent_model(x_train_dense, x_train_substituents, epochs=100)\n",
    "        # Use trained model to compute AUC scores for substructures and save them to disc.\n",
    "        actual = x_train_substituents\n",
    "        predicted = enc_basic.predict(x_train_dense)\n",
    "        base_stats, base_perm_scores = compute_auc(2098, actual, predicted, num_samples=10038)\n",
    "        \n",
    "        np.savetxt(curr_path, base_stats, fmt=['%d', '%d', '%f', '%f'])\n",
    "        experiment_stats.append(base_stats)\n",
    "    \n",
    "    evaluate(base_stats, base_perm_scores, base_stats, base_perm_scores)\n",
    "    \n",
    "    return experiment_stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The below is the basic set up for running any of the methods. It loads the fragment spectra and subtituent terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9283\n",
      "9283\n",
      "381\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "spectra = load_master_file(path=filtered_classyfire_datapath)\n",
    "substituents = load_filtered_substituents_master()\n",
    "substituent_names = load_substituent_legend(filtered_substituents_names_path)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# Train 1 time\n",
    "\n",
    "epochs = 100\n",
    "x_train_spectra = np.log(spectra+1)\n",
    "x_train_substituents = substituents\n",
    "\n",
    "enc = simplified_substituent_model(x_train_spectra, x_train_substituents.values, epochs=epochs)\n",
    "\n",
    "actual = x_train_substituents.values\n",
    "predicted = enc.predict(x_train_spectra)\n",
    "\n",
    "baseline_stats, baseline_perm_scores = compute_auc(381, actual, predicted, num_samples=9283)\n",
    "\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing base scores with experiment scores (to get the bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(baseline_stats, baseline_perm_scores, exp_stats, exp_perm_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the results in (substituent_term auc_score) format to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(substituents_auc_results_path, 'w') as f:\n",
    "    for index, auc in enumerate(exp_stats):\n",
    "        f.write(substituent_names[index] + \" \" + str(auc[2]) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to calculate how many matches are there i.e. true label == predicted label. Write to result to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substituent_dict = {}\n",
    "true_label_counter = {}\n",
    "substituent_correct_counter = {}\n",
    "\n",
    "for name in substituent_names:\n",
    "    true_label_counter[name] = 0\n",
    "    substituent_correct_counter[name] = 0\n",
    "\n",
    "# save the substituent terms indexes to a dict for each id\n",
    "# e.g. {CCMSLIB0000001 : [10, 15, 103], ...}\n",
    "with open(substituents_path, 'r') as f:\n",
    "    for line in f:\n",
    "        id, index, value = line.split(\" \")\n",
    "        if id not in substituent_dict:\n",
    "            substituent_dict[id] = []\n",
    "        substituent_dict[id].append(index)\n",
    "\n",
    "# get the index of element that has highest probability\n",
    "# for each substituent terms in the gnps id, store the max\n",
    "# in the end, go through each and see if the index is in the true labels\n",
    "# if matches, that term's counter increase\n",
    "for index, probabilities in enumerate(predicted):\n",
    "    temp = copy.deepcopy(probabilities).tolist()\n",
    "    true_labels = []\n",
    "    max_indexes = []\n",
    "    for substituent_index in substituent_dict[substituents.index[index]]:\n",
    "        true_labels.append(substituent_index)\n",
    "        max_index = temp.index(max(temp))\n",
    "        max_indexes.append(max_index)\n",
    "        temp.remove(temp[max_index])\n",
    "        true_label_counter[substituent_names[int(substituent_index)]] += 1\n",
    "    for index in max_indexes:\n",
    "        if index in true_labels:\n",
    "            substituent_correct_counter[substituent_names[int(index)]] += 1\n",
    "                    \n",
    "prediction_comparison_report_path = \"G:\\\\Dev\\\\Data\\\\substituent_prediction_comparison_report.txt\"\n",
    "with open(prediction_comparison_report_path, 'w') as f:\n",
    "    f.write(\"substituent,matched,actual,proportion\\n\")\n",
    "\n",
    "with open(prediction_comparison_report_path, 'a') as f:\n",
    "    for substituent in substituent_correct_counter:\n",
    "        matched = substituent_correct_counter[substituent]\n",
    "        actual = true_label_counter[substituent]\n",
    "        proportion = 0\n",
    "        if actual != 0:\n",
    "            proportion = matched / actual\n",
    "        f.write(substituent + \",\" + str(matched) + \",\" + str(actual) + \",\" + str(proportion*100) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method to generate validation split files to be used when training different splits 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnps_all_substituent_perm_file_path = \"G:\\\\Dev\\\\Data\\\\GNPS ALL Substituent Validation Split Permutations.txt\"\n",
    "\n",
    "# Create and store 10 randomly permuted indices for 10038 samples\n",
    "sample_size = 10038\n",
    "att = np.arange(sample_size)\n",
    "\n",
    "att = np.random.permutation(att)\n",
    "\n",
    "index_permutations = np.zeros((sample_size, 0), dtype=int)\n",
    "# Add each permutation to a numpy array of indices\n",
    "for i in range(10):\n",
    "    perm = np.random.permutation(np.arange(sample_size, dtype=int))\n",
    "    index_permutations = np.column_stack((index_permutations, perm))\n",
    "\n",
    "# Verify numpy array has correct shape (should be 5770 for each column)\n",
    "for i in range(10):\n",
    "    print(np.unique(index_permutations[:, i]).size)\n",
    "\n",
    "# Save index permutations to file\n",
    "np.savetxt(gnps_all_substituent_perm_file_path, index_permutations, delimiter=',', fmt='%d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of training 10 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HI\n"
     ]
    }
   ],
   "source": [
    "# Train 10 times (may take awhile)\n",
    "\n",
    "path = \"G:\\\\Dev\\\\Data\\\\Substituents Experiments\\\\\"\n",
    "train_diff_splits(path, \"GNPS ALL Substituents\")\n",
    "\n",
    "process_average_substituent_path = \"G:\\\\Dev\\\\Data\\\\Substituents Experiments\\\\filtered_average_result.txt\"\n",
    "substituents_legend_path = \"G:\\\\Dev\\\\Data\\\\Classyfire Taxanomy\\\\GNPS_substituents_legend.txt\"\n",
    "substituents_term_occurences_path = \"G:\\\\Dev\\\\Data\\\\Substituent Terms Occurences\\\\substituents_terms_occurences.txt\"\n",
    "variables = [\"GNPS ALL Substituents\"]\n",
    "data = []\n",
    "substituent_occurences_dict = {}\n",
    "\n",
    "# to calculate occurences\n",
    "with open(substituents_legend_path, 'r') as f:\n",
    "    content = f.readlines()\n",
    "\n",
    "with open(substituents_term_occurences_path, 'r') as f:\n",
    "    for line in f:\n",
    "        substituent, occurences = line.split(\"\\t\")\n",
    "        substituent_occurences_dict[substituent] = int(float(occurences[:-1]))\n",
    "\n",
    "for i in range(10):\n",
    "    filepath = path + variables[0] + \" \" + str(i) + \".txt\"\n",
    "    stats_one = np.loadtxt(filepath, dtype=float)\n",
    "    print(stats_one[:, 2])\n",
    "    data.append(stats_one[:, 2])\n",
    "\n",
    "print(\"Average\")\n",
    "result = np.mean(data, axis=0)\n",
    "with open(process_average_substituent_path, 'w') as f:\n",
    "    for index, probability in enumerate(result):\n",
    "        f.write(content[index][:-1] + \"\\t\" + str(probability) + \"\\t\" + str(substituent_occurences_dict[content[index][:-1]]) + \"%\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving model to h5 format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"G:\\\\Dev\\\\Data\\\\saved_substituents_classifier_model.h5\"\n",
    "enc.save(filepath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
